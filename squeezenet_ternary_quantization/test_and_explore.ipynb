{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../training_utils/')\n",
    "from data_utils import get_folders, get_class_weights\n",
    "from train_utils import predict\n",
    "from diagnostic_tools import top_k_accuracy, per_class_accuracy,\\\n",
    "    count_params, entropy, model_calibration, show_errors, most_confused_classes,\\\n",
    "    most_inaccurate_k_classes\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from get_densenet import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, val_folder = get_folders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# w[j]: 1/number_of_samples_in_class_j\n",
    "# decode: folder name to class name (in human readable format)\n",
    "w, decode = get_class_weights(val_folder.class_to_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, _, _ = get_model(class_weights=torch.FloatTensor(w/w.sum()))\n",
    "\n",
    "# load pretrained quantized model\n",
    "model.load_state_dict(torch.load('model_ternary_quantization.pytorch_state'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of params in the model\n",
    "count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show some quantized kernel tensors (there are ? such kernels overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all quantized kernels\n",
    "all_kernels = [\n",
    "    (n, p.data) for n, p in model.named_parameters() \n",
    "    if ('denseblock' in n or 'transition' in n) and 'conv' in n\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# show distribution of weights in kernels that are in the beginning of the network \n",
    "_, axes = plt.subplots(nrows=6, ncols=4, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "for i, (name, kernel) in enumerate(all_kernels[:24]):\n",
    "    axes[i].hist(kernel.cpu().numpy().reshape(-1));\n",
    "    axes[i].set_title(name[9:-7]);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kernels in the end of the network \n",
    "_, axes = plt.subplots(nrows=6, ncols=4, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "for i, (name, kernel) in enumerate(all_kernels[-24:]):\n",
    "    axes[i].hist(kernel.cpu().numpy().reshape(-1));\n",
    "    axes[i].set_title(name[9:-7]);\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sparcity distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparcity = []\n",
    "for n, p in all_kernels:\n",
    "    sparcity.append(p.eq(0.0).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(sparcity);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.mean(sparcity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scaling factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "upper_scaling_factor = []\n",
    "lower_scaling_factor = []\n",
    "for n, p in all_kernels:\n",
    "    upper_scaling_factor.append(p.max())\n",
    "    lower_scaling_factor.append(p.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(upper_scaling_factor);\n",
    "plt.hist(lower_scaling_factor);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get human readable class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# index to class name\n",
    "decode = {val_folder.class_to_idx[k]: decode[int(k)] for k in val_folder.class_to_idx}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get all predictions and all misclassified images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_iterator_no_shuffle = DataLoader(\n",
    "    val_folder, batch_size=64, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_predictions, val_true_targets,\\\n",
    "    erroneous_samples, erroneous_targets,\\\n",
    "    erroneous_predictions = predict(model, val_iterator_no_shuffle, return_erroneous=True)\n",
    "# erroneous_samples: images that were misclassified\n",
    "# erroneous_targets: their true labels\n",
    "# erroneous_predictions: predictions for them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### number of misclassified images (there are overall 5120 images in the val dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_errors = len(erroneous_targets)\n",
    "n_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logloss and accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_loss(val_true_targets, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(val_true_targets, val_predictions.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(top_k_accuracy(val_true_targets, val_predictions, k=(2, 3, 4, 5, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hits = val_predictions.argmax(1) == val_true_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    entropy(val_predictions[hits]), bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    entropy(val_predictions[~hits]), bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('entropy of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### confidence of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(\n",
    "    val_predictions[hits].max(1), bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    val_predictions[~hits].max(1), bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('confidence of predictions');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### difference between biggest and second biggest probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_correct = np.sort(val_predictions[hits], 1)\n",
    "sorted_incorrect = np.sort(val_predictions[~hits], 1)\n",
    "\n",
    "plt.hist(\n",
    "    sorted_correct[:, -1] - sorted_correct[:, -2], bins=30, \n",
    "    normed=True, alpha=0.7, label='correct prediction'\n",
    ");\n",
    "plt.hist(\n",
    "    sorted_incorrect[:, -1] - sorted_incorrect[:, -2], bins=30, \n",
    "    normed=True, alpha=0.5, label='misclassification'\n",
    ");\n",
    "plt.legend();\n",
    "plt.xlabel('difference');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### probabilistic calibration of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_calibration(val_true_targets, val_predictions, n_bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### per class accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "per_class_acc = per_class_accuracy(val_true_targets, val_predictions)\n",
    "plt.hist(per_class_acc);\n",
    "plt.xlabel('accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "most_inaccurate_k_classes(per_class_acc, 15, decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class accuracy vs. number of samples in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter((1.0/w), per_class_acc);\n",
    "plt.ylabel('class accuracy');\n",
    "plt.xlabel('number of available samples');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### most confused pairs of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "confused_pairs = most_confused_classes(\n",
    "    val_true_targets, val_predictions, decode, min_n_confusions=4\n",
    ")\n",
    "confused_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### show some low entropy errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "erroneous_entropy = entropy(erroneous_predictions)\n",
    "mean_entropy = erroneous_entropy.mean()\n",
    "low_entropy = mean_entropy < erroneous_entropy\n",
    "mean_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_errors(\n",
    "    erroneous_samples[low_entropy], \n",
    "    erroneous_predictions[low_entropy], \n",
    "    erroneous_targets[low_entropy], \n",
    "    decode\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### show some high entropy errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_errors(\n",
    "    erroneous_samples[~low_entropy], \n",
    "    erroneous_predictions[~low_entropy], \n",
    "    erroneous_targets[~low_entropy], \n",
    "    decode\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
